{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stroke Prediction Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except:\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "    \n",
    "sns.set_palette('husl')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('healthcare-dataset-stroke-data.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categorical Features:\")\n",
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of 'N/A' values in BMI: {(df['bmi'] == 'N/A').sum()}\")\n",
    "print(f\"Data type of BMI column: {df['bmi'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicate Rows Analysis:\")\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "print(f\"Percentage of duplicates: {(duplicates/len(df))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable distribution\n",
    "print(\"Target Variable (Stroke) Distribution:\")\n",
    "stroke_counts = df['stroke'].value_counts()\n",
    "stroke_percentage = df['stroke'].value_counts(normalize=True) * 100\n",
    "\n",
    "target_df = pd.DataFrame({\n",
    "    'Count': stroke_counts,\n",
    "    'Percentage': stroke_percentage\n",
    "})\n",
    "print(target_df)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=df, x='stroke', ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Stroke Cases Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Stroke (0=No, 1=Yes)', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "for container in axes[0].containers:\n",
    "    axes[0].bar_label(container)\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#90EE90', '#FF6B6B']\n",
    "axes[1].pie(stroke_counts, labels=['No Stroke', 'Stroke'], autopct='%1.1f%%', \n",
    "            startangle=90, colors=colors, explode=(0, 0.1))\n",
    "axes[1].set_title('Stroke Cases Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Class Imbalance Detected: {stroke_percentage[0]:.2f}% \")\n",
    "print(\"No Stroke vs {stroke_percentage[1]:.2f}% Stroke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numerical_cols.remove('id')\n",
    "numerical_cols.remove('stroke')\n",
    "print(\"Numerical Features:\", numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for numerical features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    if col == 'bmi':\n",
    "        data = pd.to_numeric(df[col], errors='coerce')\n",
    "    else:\n",
    "        data = df[col]\n",
    "    \n",
    "    axes[idx].hist(data.dropna(), bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col, fontsize=10)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "if len(numerical_cols) < 6:\n",
    "    for idx in range(len(numerical_cols), 6):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for numerical features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    if col == 'bmi':\n",
    "        data = pd.to_numeric(df[col], errors='coerce')\n",
    "    else:\n",
    "        data = df[col]\n",
    "    \n",
    "    axes[idx].boxplot(data.dropna(), vert=True, patch_artist=True,\n",
    "                     boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                     medianprops=dict(color='red', linewidth=2))\n",
    "    axes[idx].set_title(f'Box Plot of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel(col, fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "if len(numerical_cols) < 6:\n",
    "    for idx in range(len(numerical_cols), 6):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"Categorical Features:\", categorical_cols)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    value_counts = df[col].value_counts()\n",
    "    percentage = df[col].value_counts(normalize=True) * 100\n",
    "    result_df = pd.DataFrame({\n",
    "        'Count': value_counts,\n",
    "        'Percentage': percentage\n",
    "    })\n",
    "    print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical features\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_cols):\n",
    "    value_counts = df[col].value_counts()\n",
    "    axes[idx].bar(range(len(value_counts)), value_counts.values, \n",
    "                  color=sns.color_palette('Set3', len(value_counts)), edgecolor='black')\n",
    "    axes[idx].set_xticks(range(len(value_counts)))\n",
    "    axes[idx].set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Count', fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for i, v in enumerate(value_counts.values):\n",
    "        axes[idx].text(i, v + 100, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "if len(categorical_cols) < 6:\n",
    "    for idx in range(len(categorical_cols), 6):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy()\n",
    "df_temp['bmi'] = pd.to_numeric(df_temp['bmi'], errors='coerce')\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    sns.boxplot(data=df_temp, x='stroke', y=col, ax=axes[idx], palette='Set2')\n",
    "    axes[idx].set_title(f'{col} vs Stroke', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Stroke (0=No, 1=Yes)', fontsize=10)\n",
    "    axes[idx].set_ylabel(col, fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "if len(numerical_cols) < 6:\n",
    "    for idx in range(len(numerical_cols), 6):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison between stroke and no-stroke groups\n",
    "print(\"Mean Values Comparison by Stroke Status:\")\n",
    "comparison = df_temp.groupby('stroke')[numerical_cols].mean()\n",
    "print(comparison)\n",
    "print(\"\\nMedian Values Comparison by Stroke Status:\")\n",
    "comparison_median = df_temp.groupby('stroke')[numerical_cols].median()\n",
    "print(comparison_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features vs Stroke\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_cols):\n",
    "    ct = pd.crosstab(df[col], df['stroke'], normalize='index') * 100\n",
    "    \n",
    "    ct.plot(kind='bar', ax=axes[idx], color=['#90EE90', '#FF6B6B'], \n",
    "            edgecolor='black', alpha=0.8)\n",
    "    axes[idx].set_title(f'Stroke Rate by {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col, fontsize=10)\n",
    "    axes[idx].set_ylabel('Percentage (%)', fontsize=10)\n",
    "    axes[idx].legend(['No Stroke', 'Stroke'], loc='best')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "if len(categorical_cols) < 6:\n",
    "    for idx in range(len(categorical_cols), 6):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed stroke rate by categorical features\n",
    "print(\"Stroke Rate by Categorical Features:\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    ct = pd.crosstab(df[col], df['stroke'], margins=True)\n",
    "    ct_pct = pd.crosstab(df[col], df['stroke'], normalize='index') * 100\n",
    "    \n",
    "    result = pd.DataFrame({\n",
    "        'No Stroke Count': ct[0][:-1],\n",
    "        'Stroke Count': ct[1][:-1],\n",
    "        'Total': ct['All'][:-1],\n",
    "        'Stroke Rate (%)': ct_pct[1]\n",
    "    })\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for numerical features\n",
    "correlation_cols = numerical_cols + ['stroke']\n",
    "correlation_matrix = df_temp[correlation_cols].corr()\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            fmt='.3f', linewidths=1, square=True, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Heatmap of Numerical Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target variable\n",
    "print(\"\\nCorrelation with Stroke (Target Variable):\")\n",
    "stroke_correlation = correlation_matrix['stroke'].sort_values(ascending=False)\n",
    "print(stroke_correlation)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "stroke_correlation[stroke_correlation.index != 'stroke'].plot(kind='barh', color='teal', edgecolor='black')\n",
    "plt.title('Feature Correlation with Stroke', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Outlier Detection (IQR Method):\")\n",
    "\n",
    "outlier_summary = {}\n",
    "\n",
    "for col in numerical_cols:\n",
    "    if col == 'bmi':\n",
    "        data = pd.to_numeric(df[col], errors='coerce').dropna()\n",
    "    else:\n",
    "        data = df[col].dropna()\n",
    "    \n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    outlier_count = len(outliers)\n",
    "    outlier_percentage = (outlier_count / len(data)) * 100\n",
    "    \n",
    "    outlier_summary[col] = {\n",
    "        'Count': outlier_count,\n",
    "        'Percentage': f\"{outlier_percentage:.2f}%\",\n",
    "        'Lower Bound': f\"{lower_bound:.2f}\",\n",
    "        'Upper Bound': f\"{upper_bound:.2f}\"\n",
    "    }\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary).T\n",
    "print(outlier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KEY FINDINGS FROM EXPLORATORY DATA ANALYSIS\")\n",
    "\n",
    "print(\"1. DATASET OVERVIEW:\")\n",
    "print(f\"   • Total Records: {len(df):,}\")\n",
    "print(f\"   • Total Features: {len(df.columns)}\")\n",
    "print(f\"   • Numerical Features: {len(numerical_cols)}\")\n",
    "print(f\"   • Categorical Features: {len(categorical_cols)}\")\n",
    "\n",
    "print(\"\\n2. TARGET VARIABLE (STROKE):\")\n",
    "stroke_pct = (df['stroke'].sum() / len(df)) * 100\n",
    "print(f\"   • Stroke Cases: {df['stroke'].sum():,} ({stroke_pct:.2f}%)\")\n",
    "print(f\"   • No Stroke Cases: {len(df) - df['stroke'].sum():,} ({100-stroke_pct:.2f}%)\")\n",
    "print(f\"   • Highly Imbalanced Dataset - Will need handling\")\n",
    "\n",
    "print(\"\\n3. DATA QUALITY:\")\n",
    "bmi_missing = (df['bmi'] == 'N/A').sum()\n",
    "print(f\"   • BMI Missing Values: {bmi_missing} ({(bmi_missing/len(df))*100:.2f}%)\")\n",
    "print(f\"   • Duplicate Rows: {df.duplicated().sum()}\")\n",
    "print(f\"   • BMI stored as object (needs conversion)\")\n",
    "\n",
    "print(\"\\n4. KEY CORRELATIONS WITH STROKE:\")\n",
    "top_corr = stroke_correlation[stroke_correlation.index != 'stroke'].head(3)\n",
    "for feature, corr_value in top_corr.items():\n",
    "    print(f\"   • {feature}: {corr_value:.3f}\")\n",
    "\n",
    "print(\"\\n5. IMPORTANT OBSERVATIONS:\")\n",
    "print(\"   • Age shows strongest correlation with stroke\")\n",
    "print(\"   • Glucose level also shows positive correlation\")\n",
    "print(\"   • Hypertension and heart disease are binary indicators\")\n",
    "print(\"   • Gender, marital status, work type, and residence need encoding\")\n",
    "print(\"   • Smoking status has multiple categories including 'Unknown'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning, Preprocessing & Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "print(\"STEP 1: Handling Missing Values\")\n",
    "\n",
    "# Convert BMI from object to float, replacing 'N/A' with NaN\n",
    "print(\"\\nConverting BMI column from object to numeric...\")\n",
    "df_processed['bmi'] = pd.to_numeric(df_processed['bmi'], errors='coerce')\n",
    "print(f\"BMI data type after conversion: {df_processed['bmi'].dtype}\")\n",
    "print(f\"BMI missing values: {df_processed['bmi'].isnull().sum()}\")\n",
    "\n",
    "# Impute missing BMI values with median (robust to outliers)\n",
    "bmi_median = df_processed['bmi'].median()\n",
    "print(f\"\\nImputing {df_processed['bmi'].isnull().sum()} missing BMI values with median: {bmi_median:.2f}\")\n",
    "df_processed['bmi'].fillna(bmi_median, inplace=True)\n",
    "\n",
    "print(f\"\\nAfter imputation - BMI missing values: {df_processed['bmi'].isnull().sum()}\")\n",
    "print(\"Missing values handled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 2: Removing Unnecessary Columns\")\n",
    "\n",
    "# Remove ID column (not useful for prediction)\n",
    "print(\"\\nRemoving 'id' column (not useful for prediction)...\")\n",
    "df_processed = df_processed.drop('id', axis=1)\n",
    "\n",
    "print(f\"Columns after removal: {list(df_processed.columns)}\")\n",
    "print(f\"Shape after removal: {df_processed.shape}\")\n",
    "print(\"Unnecessary columns removed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 3: Handling Duplicate Rows\")\n",
    "\n",
    "duplicates_before = df_processed.duplicated().sum()\n",
    "print(f\"Duplicate rows before removal: {duplicates_before}\")\n",
    "\n",
    "if duplicates_before > 0:\n",
    "    df_processed = df_processed.drop_duplicates()\n",
    "    print(f\"Duplicate rows after removal: {df_processed.duplicated().sum()}\")\n",
    "    print(f\"Rows removed: {duplicates_before}\")\n",
    "    print(\"Duplicates removed!\")\n",
    "else:\n",
    "    print(\"No duplicates found!\")\n",
    "\n",
    "print(f\"\\nFinal shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"STEP 4: Encoding Categorical Variables\")\n",
    "\n",
    "categorical_columns = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Categorical columns to encode: {categorical_columns}\")\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoders = {}\n",
    "\n",
    "# Encode each categorical column\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df_processed[col] = le.fit_transform(df_processed[col])\n",
    "    label_encoders[col] = le\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Original categories: {list(le.classes_)}\")\n",
    "    print(f\"  Encoded values: {list(range(len(le.classes_)))}\")\n",
    "\n",
    "print(\"\\nAll categorical variables encoded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 5: Verification of Processed Data\")\n",
    "\n",
    "print(\"\\nProcessed Dataset Info:\")\n",
    "df_processed.info()\n",
    "\n",
    "print(\"Processed Dataset - First 5 Rows:\")\n",
    "display(df_processed.head())\n",
    "\n",
    "print(\"Statistical Summary of Processed Data:\")\n",
    "display(df_processed.describe())\n",
    "\n",
    "print(\"Missing Values After Preprocessing:\")\n",
    "print(df_processed.isnull().sum())\n",
    "\n",
    "print(\"\\nData preprocessing completed successfully!\")\n",
    "print(f\"Final dataset shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"STEP 6: Feature Scaling\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_processed.drop('stroke', axis=1)\n",
    "y = df_processed['stroke']\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform features\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame for better readability\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "print(\"\\nFeatures scaled using StandardScaler!\")\n",
    "print(\"\\nScaled Features - First 5 rows:\")\n",
    "display(X_scaled_df.head())\n",
    "\n",
    "print(\"\\nScaled Features Statistics:\")\n",
    "display(X_scaled_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Train-Test Split\")\n",
    "\n",
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nOriginal Dataset Size: {len(X_scaled)}\")\n",
    "print(f\"Training Set Size: {len(X_train)} ({(len(X_train)/len(X_scaled))*100:.1f}%)\")\n",
    "print(f\"Testing Set Size: {len(X_test)} ({(len(X_test)/len(X_scaled))*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTraining Set Distribution:\")\n",
    "print(f\"  No Stroke (0): {(y_train == 0).sum()}\")\n",
    "print(f\"  Stroke (1): {(y_train == 1).sum()}\")\n",
    "\n",
    "print(\"\\nTesting Set Distribution:\")\n",
    "print(f\"  No Stroke (0): {(y_test == 0).sum()}\")\n",
    "print(f\"  Stroke (1): {(y_test == 1).sum()}\")\n",
    "\n",
    "print(\"\\nTrain-Test split completed with stratification!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42, algorithm='SAMME'),\n",
    "    'XGBoost': XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "results = {}\n",
    "print(\"TRAINING AND EVALUATING MACHINE LEARNING MODELS\")\n",
    "print(f\"\\nTotal models to train: {len(models)}\")\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate training time\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # Calculate ROC-AUC\n",
    "    if y_pred_proba is not None:\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    else:\n",
    "        roc_auc = None\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'Model': model,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'Training Time': training_time,\n",
    "        'Predictions': y_pred,\n",
    "        'Prediction Probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n {model_name} Training Completed!\")\n",
    "    print(f\"   Training Time: {training_time:.2f} seconds\")\n",
    "    print(f\"\\n   Performance Metrics:\")\n",
    "    print(f\"   Accuracy:  {accuracy*100:.2f}%\")\n",
    "    print(f\"   Precision: {precision*100:.2f}%\")\n",
    "    print(f\"   Recall:    {recall*100:.2f}%\")\n",
    "    print(f\"   F1-Score:  {f1*100:.2f}%\")\n",
    "    if roc_auc is not None:\n",
    "        print(f\"   ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "print(\"All models trained and evaluated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy (%)': [results[model]['Accuracy'] * 100 for model in results],\n",
    "    'Precision (%)': [results[model]['Precision'] * 100 for model in results],\n",
    "    'Recall (%)': [results[model]['Recall'] * 100 for model in results],\n",
    "    'F1-Score (%)': [results[model]['F1-Score'] * 100 for model in results],\n",
    "    'ROC-AUC': [results[model]['ROC-AUC'] if results[model]['ROC-AUC'] else 0 for model in results],\n",
    "    'Training Time (s)': [results[model]['Training Time'] for model in results]\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.sort_values('Accuracy (%)', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"\\n\")\n",
    "display(comparison_df)\n",
    "\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_accuracy = comparison_df.iloc[0]['Accuracy (%)']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(f\"Accuracy: {best_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "#Accuracy Comparison\n",
    "axes[0, 0].barh(comparison_df['Model'], comparison_df['Accuracy (%)'], color='steelblue', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "for i, v in enumerate(comparison_df['Accuracy (%)']):\n",
    "    axes[0, 0].text(v + 0.5, i, f'{v:.2f}%', va='center', fontweight='bold')\n",
    "\n",
    "#Precision, Recall, F1-Score Comparison\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.25\n",
    "axes[0, 1].bar(x - width, comparison_df['Precision (%)'], width, label='Precision', color='#FF6B6B', edgecolor='black')\n",
    "axes[0, 1].bar(x, comparison_df['Recall (%)'], width, label='Recall', color='#4ECDC4', edgecolor='black')\n",
    "axes[0, 1].bar(x + width, comparison_df['F1-Score (%)'], width, label='F1-Score', color='#95E1D3', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Models', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Score (%)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Precision, Recall & F1-Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "#ROC-AUC Comparison\n",
    "axes[1, 0].barh(comparison_df['Model'], comparison_df['ROC-AUC'], color='coral', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('ROC-AUC Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('ROC-AUC Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlim([0, 1])\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "for i, v in enumerate(comparison_df['ROC-AUC']):\n",
    "    axes[1, 0].text(v + 0.02, i, f'{v:.4f}', va='center', fontweight='bold')\n",
    "\n",
    "#Training Time Comparison\n",
    "axes[1, 1].bar(comparison_df['Model'], comparison_df['Training Time (s)'], color='mediumpurple', edgecolor='black')\n",
    "axes[1, 1].set_ylabel('Time (seconds)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(comparison_df['Training Time (s)']):\n",
    "    axes[1, 1].text(i, v + 0.01, f'{v:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = len(results)\n",
    "n_rows = (n_models + 2) // 3\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(n_rows, 3, figsize=(18, 6 * n_rows))\n",
    "if n_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(y_test, result['Predictions'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[idx],\n",
    "                xticklabels=['No Stroke', 'Stroke'], yticklabels=['No Stroke', 'Stroke'])\n",
    "    axes[idx].set_title(f'{model_name}\\nAccuracy: {result[\"Accuracy\"]*100:.2f}%', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual', fontsize=10)\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=10)\n",
    "\n",
    "for idx in range(n_models, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    print(f\"\\n{model_name}\")\n",
    "    print(classification_report(y_test, result['Predictions'], \n",
    "                                target_names=['No Stroke', 'Stroke'],\n",
    "                                digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results.items()):\n",
    "    if result['Prediction Probabilities'] is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, result['Prediction Probabilities'])\n",
    "        roc_auc = result['ROC-AUC']\n",
    "        \n",
    "        plt.plot(fpr, tpr, color=colors[idx % len(colors)], lw=2, \n",
    "                label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random Classifier')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance for tree-based models\n",
    "tree_models = ['Random Forest', 'Decision Tree', 'AdaBoost','XGBoost']\n",
    "available_tree_models = [m for m in tree_models if m in results]\n",
    "\n",
    "if len(available_tree_models) > 0:\n",
    "    n_cols = 2\n",
    "    n_rows = (len(available_tree_models) + 1) // 2\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 6 * n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for idx, model_name in enumerate(available_tree_models):\n",
    "        model = results[model_name]['Model']\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            feature_names = X.columns\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importances\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            axes[idx].barh(importance_df['Feature'], importance_df['Importance'], \n",
    "                          color='teal', edgecolor='black')\n",
    "            axes[idx].set_xlabel('Importance', fontsize=10, fontweight='bold')\n",
    "            axes[idx].set_title(f'{model_name} - Feature Importance', \n",
    "                               fontsize=12, fontweight='bold')\n",
    "            axes[idx].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for idx in range(len(available_tree_models), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No tree-based models available for feature importance visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FINAL SUMMARY & RECOMMENDATIONS\")\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_result = results[best_model_name]\n",
    "\n",
    "print(f\"\\nBEST PERFORMING MODEL: {best_model_name}\")\n",
    "print(f\"   Accuracy:      {best_result['Accuracy']*100:.2f}%\")\n",
    "print(f\"   Precision:     {best_result['Precision']*100:.2f}%\")\n",
    "print(f\"   Recall:        {best_result['Recall']*100:.2f}%\")\n",
    "print(f\"   F1-Score:      {best_result['F1-Score']*100:.2f}%\")\n",
    "if best_result['ROC-AUC']:\n",
    "    print(f\"   ROC-AUC:       {best_result['ROC-AUC']:.4f}\")\n",
    "print(f\"   Training Time: {best_result['Training Time']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
